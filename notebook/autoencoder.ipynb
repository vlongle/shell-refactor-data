{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 16:47:35.060157: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-16 16:47:36.189535: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-16 16:47:36.189700: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-16 16:47:36.189708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Global seed set to 69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from lightning_lite.utilities.seed import seed_everything\n",
    "from shell_data.dataset.dataset import get_train_val_test_subsets\n",
    "import torch\n",
    "import os\n",
    "from shell_data.utils.config import (\n",
    "    ShELLDataSharingConfig,\n",
    "    DatasetConfig,\n",
    "    TaskModelConfig,\n",
    "    TrainingConfig,\n",
    "    ExperienceReplayConfig,\n",
    "    DataValuationConfig,\n",
    "    RouterConfig,\n",
    "    BoltzmanExplorationConfig,\n",
    ")\n",
    "from shell_data.utils.record import Record, snapshot_perf, snapshot_conf_mat\n",
    "import numpy as np\n",
    "from shell_data.shell_agent.shell_agent_classification import ShELLClassificationAgent\n",
    "from itertools import combinations\n",
    "import umap\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from shell_data.utils.utils import train\n",
    "import matplotlib.pyplot as plt\n",
    "# import mplcyberpunk\n",
    "# plt.style.use(\"cyberpunk\")\n",
    "# plt.style.use('bmh')\n",
    "import seaborn as sns\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "# plt.style.use(\"xkcd\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "SEED = 69\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shell_data.task_model.task_model import TaskModel, SupervisedLearningTaskModel\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cls_per_task = 5\n",
    "n_agents = 2\n",
    "num_task_per_life = 2\n",
    "buffer_integration_size = 50000  # sample all!\n",
    "batch_size = 32\n",
    "size = 64\n",
    "routing_method = \"random\"\n",
    "\n",
    "dataset_name = \"mnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subsets, val_subsets, test_subsets = get_train_val_test_subsets(\n",
    "        dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = ShELLDataSharingConfig(\n",
    "        n_agents=n_agents,\n",
    "        dataset=DatasetConfig(\n",
    "            name=dataset_name,\n",
    "            train_size=size,\n",
    "            test_size=1.0,\n",
    "            val_size=size//2,\n",
    "            num_task_per_life=num_task_per_life,\n",
    "            num_cls_per_task=num_cls_per_task,\n",
    "        ),\n",
    "        task_model=TaskModelConfig(\n",
    "            name=dataset_name,\n",
    "        ),\n",
    "        training=TrainingConfig(\n",
    "            n_epochs=50,\n",
    "            batch_size=batch_size,\n",
    "            patience=1000,\n",
    "            val_every_n_epoch=1,\n",
    "        ),\n",
    "        experience_replay=ExperienceReplayConfig(\n",
    "            buffer_size=buffer_integration_size,\n",
    "        ),\n",
    "          router=RouterConfig(\n",
    "            strategy=routing_method,  # control how the sender decides which data point to send\n",
    "            num_batches=1,\n",
    "            estimator_task_model=TaskModelConfig(\n",
    "                name=dataset_name,\n",
    "            ),\n",
    "            n_heads=n_agents,\n",
    "          ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size: 64, num_cls_per_task: 5\n"
     ]
    }
   ],
   "source": [
    "receiver = ShELLClassificationAgent(\n",
    "        train_subsets, val_subsets, test_subsets, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size: 64, num_cls_per_task: 5\n"
     ]
    }
   ],
   "source": [
    "sender_cfg = deepcopy(cfg)\n",
    "# sender_cfg.dataset.train_size = 1.0 # all of the data for testing purposes...\n",
    "sender = ShELLClassificationAgent(\n",
    "        train_subsets, val_subsets, test_subsets, sender_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ShELLClassificationAgent' object has no attribute 'init'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m sender\u001b[39m.\u001b[39mll_dataset\u001b[39m.\u001b[39mperm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m0\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m9\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m5\u001b[39m,       \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m6\u001b[39m, \u001b[39m7\u001b[39m, \u001b[39m8\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[39m# intersection be 0, 4, 9 (and 2 out of distribution!)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m receiver\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m      7\u001b[0m sender\u001b[39m.\u001b[39minit()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ShELLClassificationAgent' object has no attribute 'init'"
     ]
    }
   ],
   "source": [
    "receiver.ll_dataset.perm = torch.tensor([0, 1, 3, 4, 9,     2, 5, 6, 7, 8])\n",
    "sender.ll_dataset.perm = torch.tensor([0, 4, 9, 2, 5,       1, 3, 6, 7, 8])\n",
    "\n",
    "# intersection be 0, 4, 9 (and 2 out of distribution!)\n",
    "\n",
    "receiver.init()\n",
    "sender.init()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTAutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Auto-encoder in order to do \n",
    "    (1) image similarity search by computing the distance \n",
    "    between the latent representations.\n",
    "    (2) outlier detection by computing the reconstruction error.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes) -> None:\n",
    "        # use architecture here \n",
    "        # https://medium.com/dataseries/convolutional-autoencoder-in-pytorch-on-mnist-dataset-d65145c132ac\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),  # b, 16, 10, 10\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 5, 5\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # b, 8, 3, 3\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1),  # b, 8, 2, 2\n",
    "        )\n",
    "        # output size: 8 * 2 * 2 = 32\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # b, 16, 5, 5\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # b, 8, 15, 15\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1),  # b, 1, 28, 28\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(32, num_classes)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Optional,\n",
    "    Union,\n",
    "    Tuple,\n",
    ")\n",
    "\n",
    "class ReconstructionTaskModel(TaskModel):\n",
    "    def __init__(self):\n",
    "        self.net = MNISTAutoEncoder()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=1e-3)\n",
    "        self.device = \"cuda\"\n",
    "        self.net.to(self.device)\n",
    "    \n",
    "\n",
    "    def train_step(self, batch: Tuple[torch.Tensor, torch.Tensor], head_id=None):\n",
    "        x, _ = self.to_device(batch)\n",
    "        self.net.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        reconstructed = self.net(x)\n",
    "        loss = self.criterion(reconstructed, x)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor]):\n",
    "        x, _ = self.to_device(batch)\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            reconstructed = self.net(x)\n",
    "            loss = self.criterion(reconstructed, x)\n",
    "        return loss.item()\n",
    "    \n",
    "    def val_step(self, batch) -> float:\n",
    "        return self.test_step(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m autoencoder \u001b[39m=\u001b[39m ReconstructionTaskModel()\n",
      "Cell \u001b[0;32mIn [13], line 9\u001b[0m, in \u001b[0;36mReconstructionTaskModel.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet \u001b[39m=\u001b[39m MNISTAutoEncoder()\n\u001b[1;32m     10\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m     11\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'num_classes'"
     ]
    }
   ],
   "source": [
    "autoencoder = ReconstructionTaskModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiver_data = receiver.ll_dataset.get_train_dataset(0, kind=\"all\")\n",
    "len(receiver_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_data = sender.ll_dataset.get_train_dataset(0, kind=\"all\")\n",
    "len(sender_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    receiver_data,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_func(early_stopping, global_step, epoch, train_loss, record):\n",
    "    print(f\"Epoch: {epoch}, global_step: {global_step}, train_loss: {train_loss}\")\n",
    "    # total_train_loss = train on the entire dataset\n",
    "\n",
    "    total_dataloader = torch.utils.data.DataLoader(\n",
    "        receiver_data, batch_size=len(receiver_data), shuffle=False, num_workers=0, pin_memory=True\n",
    "    )\n",
    "    total_batch = next(iter(total_dataloader))\n",
    "    total_loss = autoencoder.test_step(total_batch)\n",
    "    record.write({\n",
    "        \"epoch\": epoch,\n",
    "        \"global_step\": global_step,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"total_loss\": total_loss\n",
    "    })\n",
    "    return early_stopping.step(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = Record(\"mnist_autoencoder.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# https://medium.com/dataseries/convolutional-autoencoder-in-pytorch-on-mnist-dataset-d65145c132ac\n",
    "# Should train for about 30 epochs\n",
    "# \"\"\"\n",
    "# if os.path.exists(\"mnist_autoencoder.pt\"):\n",
    "#     print(\"loading...\")\n",
    "#     autoencoder.net.load_state_dict(torch.load(\"mnist_autoencoder.pt\")) \n",
    "# else:\n",
    "#     print('training...')\n",
    "#     train(autoencoder, train_dataloader, val_dataloader=None, n_epochs=500, val_every_n_epoch=1,\n",
    "#         patience=20, delta=0.0,\n",
    "#         val_func=partial(val_func, record=record), val_before=False);\n",
    "#     torch.save(autoencoder.net.state_dict(), \"mnist_autoencoder.pt\")\n",
    "#     record.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(f\"mnist_autoencoder.csv\")\n",
    "# # shortened = df.iloc[15:]\n",
    "# df.plot(x=\"epoch\", y=[\"train_loss\", \"total_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"total_loss\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pick some random images and see how the autoencoder reconstructs them\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# num_images = 10\n",
    "\n",
    "# rand_idx = np.random.randint(0, len(receiver_data), num_images)\n",
    "# images = [receiver_data[i][0] for i in rand_idx]\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=num_images, figsize=(20, 4))\n",
    "# for i, image in enumerate(images):\n",
    "#     axes[0, i].imshow(image.squeeze(), cmap=\"gray\")\n",
    "#     axes[0, i].axis(\"off\")\n",
    "#     axes[1, i].imshow(autoencoder.net(image.unsqueeze(0).to(autoencoder.device)).squeeze().cpu().detach(), cmap=\"gray\")\n",
    "#     axes[1, i].axis(\"off\")\n",
    "\n",
    "# plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruction looks pretty good! Reconstruction error is about 0.02, which is what you'd expect for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_features(X):\n",
    "    return X.view(X.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clustering_reducer(reducer, X):\n",
    "    return torch.tensor(reducer.transform(to_features(X).cpu().numpy()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe need other metric:\n",
    "https://medium.com/analytics-vidhya/image-similarity-model-6b89a22e2f1a\n",
    "(add tsne embedding on top of the encoded X) or use cosine similarity\n",
    "instead of 2D distance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_reducer(autoencoder, X):\n",
    "    X = X.to(autoencoder.device)\n",
    "    X = autoencoder.net.encoder(X)\n",
    "    X = to_features(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def image_search(queries, database, reducer_callable, n_neighbors=10, p=2, metric=\"distance\"):\n",
    "    query_embed = reducer_callable(X=queries)\n",
    "    database_embed = reducer_callable(X=database)\n",
    "    if metric == \"distance\":\n",
    "        dist = torch.cdist(query_embed, database_embed, p=p)\n",
    "    elif metric == \"cosine\":\n",
    "        dist = 1 - torch.stack([F.cosine_similarity(query_embed[i], database_embed) for i in range(len(query_embed))])\n",
    "    else:\n",
    "        raise ValueError(f\"metric {metric} is not supported\")\n",
    "    closest_dist, closest_idx = torch.topk(dist, k=n_neighbors, dim=1, largest=False)\n",
    "    return closest_dist, closest_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_receiver = torch.stack([x for x, _ in receiver_data])\n",
    "X_sender = torch.stack([x for x, _ in sender_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sender = torch.tensor([y for _, y in sender_data])\n",
    "y_receiver = torch.tensor([y for _, y in receiver_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_receiver"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pick some random images from receiver and plot the neighbors from sender\n",
    "returned above\n",
    "\"\"\"\n",
    "\n",
    "def viz_image_search(queries, database, closest_idx, closest_dist):\n",
    "    num_images = 10\n",
    "    n_neighbors = 10\n",
    "    x_idx = np.random.randint(0, len(queries), num_images)\n",
    "    x = queries[x_idx]\n",
    "    closest_idx = closest_idx[x_idx]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=num_images, ncols=n_neighbors + 1, figsize=( 10, 12));\n",
    "    for i, image in enumerate(x):\n",
    "        axes[i, 0].imshow(image.cpu().squeeze(), cmap=\"gray\");\n",
    "        axes[i, 0].axis(\"off\");\n",
    "        for j in range(10):\n",
    "            axes[i, j+1].imshow(database[closest_idx[i, j]].cpu().squeeze(), cmap=\"gray\");\n",
    "            axes[i, j+1].title.set_text(f\"{closest_dist[i, j]:.2f}\");\n",
    "            axes[i, j+1].axis(\"off\");\n",
    "    \n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_img_search_quality(queries, database, query_y, database_y, closest_idx, n_neighbors=5):\n",
    "    # for each query get the k nearest neighbors\n",
    "    closest_idx = closest_idx.cpu()\n",
    "    neighbor_idx = closest_idx[:, :n_neighbors]\n",
    "    # compute the accuracy which is defined the fraction of database_y of neighbor_idx match with query_y\n",
    "    neighbor_y = database_y[neighbor_idx]\n",
    "    accuracy = (neighbor_y == query_y.unsqueeze(1)).sum(dim=1) / n_neighbors\n",
    "    # accuracy per sample\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=10, min_dist=0.0, n_components=2, random_state=42)\n",
    "reducer.fit(to_features(X_receiver).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "closest_dist, closest_idx = image_search(X_receiver, X_receiver, reducer_callable=partial(autoencoder_reducer, autoencoder=autoencoder),\n",
    "metric=\"cosine\")\n",
    "viz_image_search(X_receiver, X_receiver, closest_idx, closest_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_training_retrieval_acc = compute_img_search_quality(X_receiver, X_receiver, y_receiver, y_receiver, closest_idx)\n",
    "print(ae_training_retrieval_acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_dist, closest_idx = image_search(X_receiver, X_receiver, reducer_callable=partial(clustering_reducer, reducer=reducer))\n",
    "viz_image_search(X_receiver, X_receiver, closest_idx, closest_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_training_retrieval_acc = compute_img_search_quality(X_receiver, X_receiver, y_receiver, y_receiver, closest_idx)\n",
    "print(clustering_training_retrieval_acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_dist, closest_idx = image_search(X_sender, X_receiver,  reducer_callable=partial(autoencoder_reducer, autoencoder=autoencoder), metric=\"cosine\")\n",
    "viz_image_search(X_sender, X_receiver, closest_idx, closest_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_testing_retrieval_acc = compute_img_search_quality(X_sender, X_receiver, y_sender, y_receiver, closest_idx)\n",
    "print(ae_testing_retrieval_acc.mean())\n",
    "# filter out all rows that have 2 and 5 labels (OOD)\n",
    "in_dist_testing_acc = ae_testing_retrieval_acc[(y_sender != 2) & (y_sender != 5)]\n",
    "print(in_dist_testing_acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_dist, closest_idx = image_search(X_sender, X_receiver,  reducer_callable=partial(clustering_reducer, reducer=reducer))\n",
    "viz_image_search(X_sender, X_receiver, closest_idx, closest_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_testing_retrieval_acc = compute_img_search_quality(X_sender, X_receiver, y_sender, y_receiver, closest_idx)\n",
    "print(clustering_testing_retrieval_acc.mean())\n",
    "# filter out all rows that have 2 and 5 labels (OOD)\n",
    "in_dist_testing_acc = clustering_testing_retrieval_acc[(y_sender != 2) & (y_sender != 5)]\n",
    "print(in_dist_testing_acc.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the loss distribution of the autoencoder on the training data\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    receiver_data,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "losses = []\n",
    "for batch in train_dataloader:\n",
    "    losses.append(autoencoder.test_step(batch))\n",
    "len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the upper bound of the loss distribution as k-standard deviations away from the mean\n",
    "k = 3\n",
    "mean = np.mean(losses)\n",
    "std = np.std(losses)\n",
    "upper_bound = mean + k * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss distribution\n",
    "plt.hist(losses, bins=100);\n",
    "# draw a vertical line at the upper bound\n",
    "plt.axvline(x=upper_bound, color=\"red\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers(model, x, upper_bound):\n",
    "    # x = (batch_size, 1, 28, 28)\n",
    "    x = x.to(model.device)\n",
    "    model.net.eval()\n",
    "    criterion = nn.MSELoss(reduce=False)\n",
    "    with torch.no_grad():\n",
    "        reconstructed = model.net(x)\n",
    "        loss = criterion(reconstructed, x).mean(dim=(1, 2, 3))\n",
    "    return (loss > upper_bound).cpu(), loss.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_outliers(outliers, losses, outlier_idx, X):\n",
    "    n_samples = min(10, len(outliers))\n",
    "    print(f\"Found {len(outliers)} outliers\")\n",
    "    # plot the outliers\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=n_samples, figsize=(20, 2));\n",
    "    for i in range(n_samples):\n",
    "        if n_samples < len(outliers):\n",
    "            random_idx = np.random.randint(0, len(outliers))\n",
    "        else:\n",
    "            random_idx = i\n",
    "        axes[0, i].imshow(X[outlier_idx][random_idx].cpu().squeeze(), cmap=\"gray\");\n",
    "        axes[0, i].title.set_text(f\"{losses[outlier_idx][random_idx]:.4f}\");\n",
    "        # plot the reconstructed image\n",
    "        axes[1, i].imshow(autoencoder.net(X[outlier_idx][random_idx].unsqueeze(0).to(autoencoder.device)).squeeze().cpu().detach(), cmap=\"gray\");\n",
    "        axes[0, i].axis(\"off\");\n",
    "        axes[1, i].axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiver_outliers_idx, losses = get_outliers(autoencoder, X_receiver, upper_bound)\n",
    "receiver_outliers = X_receiver[receiver_outliers_idx]\n",
    "viz_outliers(receiver_outliers, losses, receiver_outliers_idx, X_receiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(y_receiver[receiver_outliers_idx], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_outliers_idx, losses = get_outliers(autoencoder, X_sender, upper_bound)\n",
    "sender_outliers = X_sender[sender_outliers_idx]\n",
    "print(len(sender_outliers))\n",
    "print(min(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(y_sender[sender_outliers_idx], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_outliers(sender_outliers, losses, sender_outliers_idx, X_sender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the ae image similarity again, this time with the outlier filter...\n",
    "closest_dist, closest_idx = image_search(X_sender, X_receiver,  reducer_callable=partial(autoencoder_reducer, autoencoder=autoencoder))\n",
    "viz_image_search(X_sender, X_receiver, closest_idx, closest_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_testing_retrieval_acc = compute_img_search_quality(X_sender, X_receiver, y_sender, y_receiver, closest_idx)\n",
    "print(ae_testing_retrieval_acc.mean())\n",
    "# filter out all rows that have 2 and 5 labels (OOD)\n",
    "out_dist_testing_acc = ae_testing_retrieval_acc[sender_outliers_idx]\n",
    "in_dist_testing_acc = ae_testing_retrieval_acc[~sender_outliers_idx]\n",
    "print(out_dist_testing_acc.mean())\n",
    "print(in_dist_testing_acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrasive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.06, contrast_mode='one',\n",
    "                 base_temperature=0.06):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float()\n",
    "        else:\n",
    "            mask = mask.float()\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "        # compute log_prob\n",
    "        logits = torch.clamp(logits, min=-20)\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        exp_logits = torch.where(exp_logits > 1e-6, exp_logits, torch.tensor(0).float())\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-6)\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    receiver_data,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTContrastiveEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Auto-encoder in order to do \n",
    "    (1) image similarity search by computing the distance \n",
    "    between the latent representations.\n",
    "    (2) outlier detection by computing the reconstruction error.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        # use architecture here \n",
    "        # https://medium.com/dataseries/convolutional-autoencoder-in-pytorch-on-mnist-dataset-d65145c132ac\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),  # b, 16, 10, 10\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 5, 5\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # b, 8, 3, 3\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1),  # b, 8, 2, 2\n",
    "        )\n",
    "         # output size: 8 * 2 * 2 = 32\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # b, 16, 5, 5\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # b, 8, 15, 15\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1),  # b, 1, 28, 28\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "network = MNISTContrastiveEncoder()\n",
    "losses = []\n",
    "cont_losses = []\n",
    "rec_losses = []\n",
    "\n",
    "n_epochs = 1\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "\n",
    "scl = SupConLoss()\n",
    "rl = nn.MSELoss()\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        x, y = batch\n",
    "        train_transform = transforms.Compose([\n",
    "                        # transforms.RandomResizedCrop(size=32, scale=(0.2, 1.)),\n",
    "                        transforms.RandomResizedCrop(size=28, scale=(0.2, 1.)),\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        transforms.RandomApply([\n",
    "                            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "                        ], p=0.8),\n",
    "                        transforms.RandomGrayscale(p=0.2),\n",
    "                    ])\n",
    "        print(x.shape, train_transform(x).shape)\n",
    "\n",
    "        encoded_transformed_images = network.encoder(train_transform(x))\n",
    "        encoded_images = network.encoder(x)\n",
    "        print(encoded_transformed_images.shape, encoded_images.shape)\n",
    "\n",
    "        encoded_transformed_images = encoded_transformed_images.view(\n",
    "            encoded_transformed_images.shape[0], -1)\n",
    "        encoded_images = encoded_images.view(\n",
    "            encoded_images.shape[0], -1)\n",
    "\n",
    "        features = torch.cat(\n",
    "            [encoded_transformed_images.unsqueeze(1), \n",
    "                encoded_images.unsqueeze(1)], dim=1)\n",
    "\n",
    "        cont_loss = scl(features, y)\n",
    "        rec_loss = rl(x, network(x))\n",
    "        loss = cont_loss + rec_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        cont_losses.append(cont_loss.item())\n",
    "        rec_losses.append(rec_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses);\n",
    "plt.plot(cont_losses, color='red');\n",
    "plt.plot(rec_losses, color='green');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cont_losses, color='red');\n",
    "print(min(cont_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rec_losses, color='green');\n",
    "print(min(rec_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_callable(network, X):\n",
    "    \"\"\"\n",
    "    This function is called by the ContrastiveModel\n",
    "    \"\"\"\n",
    "    encoded_images = network.encoder(X).view(X.shape[0], -1)\n",
    "    return encoded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "closest_dist, closest_idx = image_search(X_receiver, X_receiver, reducer_callable=partial(contrastive_callable, network=network),\n",
    "metric=\"cosine\")\n",
    "viz_image_search(X_receiver, X_receiver, closest_idx, closest_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_training_retrieval_acc = compute_img_search_quality(X_receiver, X_receiver, y_receiver, y_receiver, closest_idx)\n",
    "print(contrastive_training_retrieval_acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_dist, closest_idx = image_search(X_sender, X_receiver,  reducer_callable=partial(contrastive_callable, network=network),\n",
    "metric=\"cosine\")\n",
    "viz_image_search(X_sender, X_receiver, closest_idx, closest_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_testing_retrieval_acc = compute_img_search_quality(X_sender, X_receiver, y_sender, y_receiver, closest_idx)\n",
    "print(contrastive_testing_retrieval_acc.mean())\n",
    "# filter out all rows that have 2 and 5 labels (OOD)\n",
    "in_dist_testing_acc = contrastive_testing_retrieval_acc[(y_sender != 2) & (y_sender != 5)]\n",
    "print(in_dist_testing_acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the loss distribution of the autoencoder on the training data\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    receiver_data,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "losses = []\n",
    "for batch in train_dataloader:\n",
    "    x, y = batch\n",
    "    train_transform = transforms.Compose([\n",
    "                    transforms.RandomResizedCrop(size=32, scale=(0.2, 1.)),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomApply([\n",
    "                        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "                    ], p=0.8),\n",
    "                    transforms.RandomGrayscale(p=0.2),\n",
    "                ])\n",
    "\n",
    "    encoded_transformed_images = network.encoder(train_transform(x))\n",
    "    encoded_images = network.encoder(x)\n",
    "\n",
    "    encoded_transformed_images = encoded_transformed_images.view(\n",
    "        encoded_transformed_images.shape[0], -1)\n",
    "    encoded_images = encoded_images.view(\n",
    "        encoded_images.shape[0], -1)\n",
    "\n",
    "    features = torch.cat(\n",
    "        [encoded_transformed_images.unsqueeze(1), \n",
    "            encoded_images.unsqueeze(1)], dim=1)\n",
    "\n",
    "    cont_loss = scl(features, y)\n",
    "    rec_loss = rl(x, network(x))\n",
    "    loss = cont_loss + rec_loss\n",
    "    losses.append(loss.item())\n",
    "len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the upper bound of the loss distribution as k-standard deviations away from the mean\n",
    "k = 2\n",
    "mean = np.mean(losses)\n",
    "std = np.std(losses)\n",
    "upper_bound = mean + k * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss distribution\n",
    "plt.hist(losses, bins=100);\n",
    "# draw a vertical line at the upper bound\n",
    "plt.axvline(x=upper_bound, color=\"red\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers(model, x, upper_bound):\n",
    "    # x = (batch_size, 1, 28, 28)\n",
    "    # x = x.to(model.device)\n",
    "    criterion = nn.MSELoss()\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss(reduce=False)\n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(x)\n",
    "        loss = criterion(reconstructed, x).mean(dim=(1, 2, 3))\n",
    "    return (loss > upper_bound).cpu(), loss.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_outliers(outliers, losses, outlier_idx, X):\n",
    "    n_samples = min(10, len(outliers))\n",
    "    print(f\"Found {len(outliers)} outliers\")\n",
    "    # plot the outliers\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=n_samples, figsize=(20, 2));\n",
    "    for i in range(n_samples):\n",
    "        if n_samples < len(outliers):\n",
    "            random_idx = np.random.randint(0, len(outliers))\n",
    "        else:\n",
    "            random_idx = i\n",
    "        axes[0, i].imshow(X[outlier_idx][random_idx].cpu().squeeze(), cmap=\"gray\");\n",
    "        axes[0, i].title.set_text(f\"{losses[outlier_idx][random_idx]:.4f}\");\n",
    "        # plot the reconstructed image\n",
    "        axes[1, i].imshow(autoencoder.net(X[outlier_idx][random_idx].unsqueeze(0).to(autoencoder.device)).squeeze().cpu().detach(), cmap=\"gray\");\n",
    "        axes[0, i].axis(\"off\");\n",
    "        axes[1, i].axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiver_outliers_idx, losses = get_outliers(network, X_receiver, upper_bound)\n",
    "receiver_outliers = X_receiver[receiver_outliers_idx]\n",
    "viz_outliers(receiver_outliers, losses, receiver_outliers_idx, X_receiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_ood.detector import (\n",
    "     ODIN,\n",
    "     EnergyBased,\n",
    "     KLMatching,\n",
    "     Mahalanobis,\n",
    "     MaxLogit,\n",
    "     MaxSoftmax,\n",
    "     ViM,\n",
    "     MCD,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiver.load_model(\"./results/ood.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap receiver_data so that y classes are 0, 1, 2, 3, 4\n",
    "receiver.model.net.to(\"cpu\")\n",
    "y_receiver_remap = torch.zeros_like(y_receiver)\n",
    "for i, y in enumerate(torch.unique(y_receiver)):\n",
    "    y_receiver_remap[y_receiver == y] = i\n",
    "\n",
    "receiver_data_remapped = torch.utils.data.TensorDataset(X_receiver, y_receiver_remap)\n",
    "\n",
    "detector = Mahalanobis(receiver.model.net.features)\n",
    "receiver_dataloader = torch.utils.data.DataLoader(\n",
    "    receiver_data_remapped,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "detector.fit(receiver_dataloader, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "with torch.no_grad():\n",
    "    train_scores = detector(X_receiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "# higher values indicate more likely to be OOD\n",
    "m = torch.mean(train_scores.cpu())\n",
    "print(\"mean\", m)\n",
    "plt.hist(train_scores.cpu().numpy(), bins=100);\n",
    "\n",
    "k = 0.5\n",
    "upper_bound = m + k * torch.std(train_scores.cpu())\n",
    "print(\"upper bound\", upper_bound)\n",
    "plt.axvline(m, color=\"green\", label=\"mean\")\n",
    "plt.axvline(upper_bound, color=\"red\", label=\"upper bound\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "with torch.no_grad():\n",
    "    test_scores = detector(X_sender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_outliers_idx = (test_scores > upper_bound).cpu()\n",
    "print(\"Number of outliers:\", sender_outliers_idx.sum())\n",
    "y_outlier = y_sender[sender_outliers_idx]\n",
    "sender_outliers = X_sender[sender_outliers_idx]\n",
    "print(\"Outlier labels:\", torch.unique(y_outlier, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_classes = [2, 5]\n",
    "num_total_outliers = len(outlier_classes) * 64\n",
    "num_outliers_caught = 0\n",
    "for clz in outlier_classes:\n",
    "    num_outliers_caught += (y_outlier == clz).sum().item() \n",
    "\n",
    "acc = num_outliers_caught / len(y_outlier)\n",
    "false_positive_rate = 1 - acc\n",
    "detection_rate = num_outliers_caught / num_total_outliers\n",
    "print(\"acc:\", acc)\n",
    "print(\"detecion rate:\", detection_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sender_outliers = X_sender[sender_outliers_idx]\n",
    "# print(len(sender_outliers))\n",
    "# print(min(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(y_sender[sender_outliers_idx], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_outliers(sender_outliers, losses, sender_outliers_idx, X_sender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_testing_retrieval_acc = compute_img_search_quality(X_sender, X_receiver, y_sender, y_receiver, closest_idx)\n",
    "print(contrastive_testing_retrieval_acc.mean())\n",
    "# filter out all OOD rows (according to the estimated OOD detector)\n",
    "in_dist_testing_acc = contrastive_testing_retrieval_acc[~sender_outliers_idx]\n",
    "print(in_dist_testing_acc.shape, in_dist_testing_acc.mean())\n",
    "out_dist_testing_acc = contrastive_testing_retrieval_acc[sender_outliers_idx]\n",
    "print(out_dist_testing_acc.shape, out_dist_testing_acc.mean())\n",
    "in_dist_testing_acc = contrastive_testing_retrieval_acc[(y_sender != 2) & (y_sender != 5)]\n",
    "print(\"oracle\", in_dist_testing_acc.shape, in_dist_testing_acc.mean())\n",
    "out_dist_testing_acc = contrastive_testing_retrieval_acc[(y_sender == 2) | (y_sender == 5)]\n",
    "print(\"oracle\", out_dist_testing_acc.shape, out_dist_testing_acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-sharing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ce37e01e4e25dad4acdf5a835a9fd4c67494d5c7616f20984ba493f320ae940"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
